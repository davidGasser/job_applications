# Use NVIDIA CUDA base image with Ubuntu 22.04
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install Python 3.11 and dependencies
RUN apt-get update && apt-get install -y \
    software-properties-common \
    wget \
    git \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Upgrade pip
RUN pip3 install --upgrade pip

# Install llama-cpp-python with CUDA support (latest version for Qwen3-VL support)
RUN CMAKE_ARGS="-DLLAMA_CUDA=on" pip3 install --upgrade llama-cpp-python[server]

# Create directory for models
RUN mkdir -p /models

# Set working directory
WORKDIR /models

# Download Llama 3.2 3B for text generation (job scoring)
# Vision models (Qwen3-VL) are handled by Ollama in the hybrid setup
RUN wget -q --show-progress \
    https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf \
    -O /models/llama-3.2-3b-instruct.gguf

# Expose port 11434
EXPOSE 11434

# Start llama-cpp-python server with parallel processing
# --parallel 4: Number of slots for parallel request processing
# --n_gpu_layers -1: Offload all layers to GPU (-1 = all layers)
# --n_ctx 4096: Context window size
# --host 0.0.0.0: Listen on all interfaces
# --port 11434: Port for the server
    CMD ["python3", "-m", "llama_cpp.server", "--model", "/models/llama-3.2-3b-instruct.gguf", "--host", "0.0.0.0", "--port", "11434", "--flash_attn", "True", "--n_gpu_layers", "-1", "--cache", "True", "--cache_type", "ram","--cache_size", "2147483648", "--n_ctx", "8192", "--n_batch", "512"]
