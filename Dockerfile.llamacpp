# Use NVIDIA CUDA base image with Ubuntu 22.04
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install Python 3.11 and dependencies
RUN apt-get update && apt-get install -y \
    software-properties-common \
    wget \
    git \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Upgrade pip
RUN pip3 install --upgrade pip

# Install llama-cpp-python with CUDA support (latest version for Qwen3-VL support)
RUN CMAKE_ARGS="-DLLAMA_CUDA=on" pip3 install --upgrade llama-cpp-python[server]

# Create directory for models
RUN mkdir -p /models

# Set working directory
WORKDIR /models

# Download Llama 3.2 3B for text generation (job scoring)
# Vision models (Qwen3-VL) are handled by Ollama in the hybrid setup
RUN wget -q --show-progress \
    https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf \
    -O /models/llama-3.2-3b-instruct.gguf

# Expose port 11434 (same as Ollama for compatibility)
EXPOSE 11434

# Start llama-cpp-python server with parallel processing
# --n-parallel 4: Process up to 4 requests concurrently
# --n-gpu-layers -1: Offload all layers to GPU
# --n-ctx 4096: Context window size
# --host 0.0.0.0: Listen on all interfaces
# --port 11434: Match Ollama port
CMD ["python3", "-m", "llama_cpp.server", \
     "--model", "/models/llama-3.2-3b-instruct.gguf", \
     "--host", "0.0.0.0", \
     "--port", "11434", \
     "--n-gpu-layers", "-1", \
     "--n-parallel", "4", \
     "--n-ctx", "4096"]
