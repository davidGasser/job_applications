services:
  # db:
  #   image: postgres:16-alpine
  #   restart: always
  #   environment:
  #     - POSTGRES_USER=user
  #     - POSTGRES_PASSWORD=password
  #     - POSTGRES_DB=jobs
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data/
  #   ports:
  #     - '5433:5432'
  # app:
  #   build: 
  #     context: .
  #     dockerfile: Dockerfile
  #   restart: always
  #   ports:
  #     - '5000:5000'
  #     - '5678:5678'
  #   volumes:
  #     - .:/app
  #   depends_on:
  #     - db
  #     - selenium
  #     # - llamacpp
  #     - ollama
  #   env_file:
  #     - .env
  #   environment:
  #     - TZ=Europe/Berlin
  #     - DATABASE_URL=postgresql://user:password@db:5432/jobs
  #     - FLASK_APP=src/main.py
  #     - FLASK_DEBUG=1
  #     - ENABLE_DEBUGPY=1
  #     # - LLAMACPP_HOST=http://llamacpp:11434
  #     - OLLAMA_HOST=http://ollama:11434
  #   command: python -Xfrozen_modules=off src/main.py

  # selenium:
  #   image: selenium/standalone-chrome:latest
  #   shm_size: 2g
  #   restart: always
  #   ports:
  #     - '4444:4444'

  dev:
    build: .
    container_name: dev
    volumes:
      - .:/app
    ports:
      - "5678:5678"
    working_dir: /app
    stdin_open: true
    tty: true
    command: /bin/bash
    depends_on:
      - llama-cpp-server
      - ollama
    environment:
      - LLAMA_CPP_HOST=http://llama-cpp-server:11434
      - OLLAMA_HOST=http://ollama:11434

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11435:11434"  # Map to port 11435 externally to avoid conflict with llama-cpp
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_FLASH_ATTENTION=true
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_NUM_PARALLEL=2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  llama-cpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-cpp-server
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    ports:
      - "11434:11434"
    
    volumes:
      - ./models:/models:ro
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    command: >
      --host 0.0.0.0
      --port 11434
      --model /models/Qwen3-4B-Q4_K_M.gguf
      --ctx-size 32768
      --n-gpu-layers -1
      --jinja
      --chat-template chatml
      --flash-attn on 
      --temp 0.6
      --top-k 20 
      --top-p 0.9 
      --min-p 0
      --repeat-penalty 1.1
      --metrics
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    restart: unless-stopped


volumes:
  postgres_data:
  ollama:
